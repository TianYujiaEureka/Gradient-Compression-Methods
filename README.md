# Gradient-Compression-Methods

Pytorch version code for papers:

1、Wangni J, Wang J, Liu J, et al. Gradient sparsification for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2018: 1299-1309.     http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf

2、Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.    https://arxiv.org/pdf/1610.02132.pdf

3、Stich, Sebastian U., Jean-Baptiste Cordonnier, and Martin Jaggi. "Sparsified SGD with memory." Advances in Neural Information Processing Systems. 2018.   http://papers.nips.cc/paper/7697-sparsified-sgd-with-memory.pdf

4、Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed optimisation for non-convex problems[J]. arXiv preprint arXiv:1802.04434, 2018.     https://arxiv.org/pdf/1802.04434.pdf

